{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":32521,"sourceType":"datasetVersion","datasetId":25491},{"sourceId":937769,"sourceType":"datasetVersion","datasetId":507452}],"dockerImageVersionId":29297,"isInternetEnabled":true,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n---\ntitle: \"**Star Wars Scripts Analysis**\"\nauthor: Juhi Pathak\ndate: '`r Sys.Date()`'\noutput: \n  html_document:\n    number_sections: yes\n    toc: yes\n    theme: cosmo\n    highlight: tango\n---\n\n# **Introduction** \n\nIn this kernel we are going to perform a statistical text analysis on the Star Wars scripts from The Original Trilogy Episodes (IV, V and VI), using wordclouds to show the most frequent words. The input files used for the analysis are avaliable  [here](https://github.com/gastonstat/StarWars). This post is my particular tribute to the [Star Wars Day](https://en.wikipedia.org/wiki/Star_Wars_Day), on May 4.\n\n# **Loading data** {.tabset .tabset-fade .tabset-pills}\n\n```{r message=FALSE, warning=FALSE}\n# Load libraries\ninstall.packages(\"RWeka\") # install RWeka package\nlibrary(tidyverse) # data manipulation\nlibrary(tm) # text mining\nlibrary(wordcloud) # word cloud generator\nlibrary(wordcloud2) # word cloud generator\nlibrary(tidytext) # text mining for word processing and sentiment analysis\nlibrary(reshape2) # reshapes a data frame\nlibrary(radarchart) # drawing the radar chart from a data frame\nlibrary(RWeka) # data mining tasks\nlibrary(knitr) # dynamic report generation\n\n# Read the data\nep4 <- read.table(\"../input/star-wars-movie-scripts/SW_EpisodeIV.txt\")\nep5 <- read.table(\"../input/star-wars-movie-scripts/SW_EpisodeV.txt\")\nep6 <- read.table(\"../input/star-wars-movie-scripts/SW_EpisodeVI.txt\")\n\n# Read the Lexicons (for sentiment classification)\nbing <- read_csv(\"../input/bing-nrc-afinn-lexicons/Bing.csv\")\nnrc <- read_csv(\"../input/bing-nrc-afinn-lexicons/NRC.csv\")\nafinn <- read_csv(\"../input/bing-nrc-afinn-lexicons/Afinn.csv\")\n```\n\nLet’s get an idea of what we’re working with.\n\n## Episode IV\n```{r message=FALSE, warning=FALSE}\n# Structure\nstr(ep4)\n```\n\n## Episode V\n```{r message=FALSE, warning=FALSE}\n# Structure\nstr(ep5)\n```\n\n## Episode VI\n```{r message=FALSE, warning=FALSE}\n# Structure\nstr(ep6)\n```\n\n# **Functions**\n\nThe first function performs cleaning and preprocessing steps to a corpus:\n\n* `removePunctuation()`. Remove all punctuation marks\n* `stripWhitespace()`. Remove excess whitespace\n* `tolower()`. Make all characters lowercase\n* `removeWords()`. Remove some common English stop words (\"I\", \"she'll\", \"the\", \"don't\" etc.)\n* `removeNumbers()`. Remove numbers \n\n```{r message=FALSE, warning=FALSE}\n# Text transformations\ncleanCorpus <- function(corpus){\n\n  corpus.tmp <- tm_map(corpus, removePunctuation)\n  corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)\n  corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))\n  v_stopwords <- c(stopwords(\"english\"), c(\"thats\",\"weve\",\"hes\",\"theres\",\"ive\",\"im\",\n                                           \"will\",\"can\",\"cant\",\"dont\",\"youve\",\"us\",\n                                           \"youre\",\"youll\",\"theyre\",\"whats\",\"didnt\"))\n  corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)\n  corpus.tmp <- tm_map(corpus.tmp, removeNumbers)\n  return(corpus.tmp)\n\n}\n```\n\nThe second function constructs the term-document matrix, that describes the frequency of terms that occur in a collection of documents. This matrix has terms in the first column and documents across the top as individual column names.\n\n```{r message=FALSE, warning=FALSE}\n# Most frequent terms \nfrequentTerms <- function(text){\n\n  s.cor <- Corpus(VectorSource(text))\n  s.cor.cl <- cleanCorpus(s.cor)\n  s.tdm <- TermDocumentMatrix(s.cor.cl)\n  s.tdm <- removeSparseTerms(s.tdm, 0.999)\n  m <- as.matrix(s.tdm)\n  word_freqs <- sort(rowSums(m), decreasing=TRUE)\n  dm <- data.frame(word=names(word_freqs), freq=word_freqs)\n  return(dm)\n\n}\n```\n\nThe next two functions extract tokens containing two words. \n\n```{r message=FALSE, warning=FALSE}\n# Define bigram tokenizer \ntokenizer  <- function(x){\n\n  NGramTokenizer(x, Weka_control(min=2, max=2))\n\n}\n```\n\n```{r message=FALSE, warning=FALSE}\n# Most frequent bigrams \nfrequentBigrams <- function(text){\n\n  s.cor <- VCorpus(VectorSource(text))\n  s.cor.cl <- cleanCorpus(s.cor)\n  s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer))\n  s.tdm <- removeSparseTerms(s.tdm, 0.999)\n  m <- as.matrix(s.tdm)\n  word_freqs <- sort(rowSums(m), decreasing=TRUE)\n  dm <- data.frame(word=names(word_freqs), freq=word_freqs)\n  return(dm)\n\n}\n```\n\n# **Episode IV: A New Hope**\n\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# How many dialogues?\nlength(ep4$dialogue)\n\n# How many characters?\nlength(levels(ep4$character))\n\n# Top 20 characters with more dialogues \ntop.ep4.chars <- as.data.frame(sort(table(ep4$character), decreasing=TRUE))[1:20,]\n\n# Visualization \nggplot(data=top.ep4.chars, aes(x=Var1, y=Freq)) +\n  geom_bar(stat=\"identity\", fill=\"#56B4E9\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Character\", y=\"Number of dialogues\")\n```\n\n```{r eval=FALSE, message=FALSE, warning=FALSE}\n# Wordcloud for Episode IV\nwordcloud2(frequentTerms(ep4$dialogue), size=0.5,\n           figPath=\"../input/wordcloud_masks/vader.png\")\n```\n\n<img src=\"https://i.imgur.com/TzUfUrQ.png\">\n\n**NOTE**: I've had a lot of problems with the renderization of the wordclouds in Kaggle. In order to solve it, I have exported the images from RStudio and I have published them in [Imgur](https://imgur.com/), using the URLs in this kernel.     \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Most frequent bigrams\nep4.bigrams <- frequentBigrams(ep4$dialogue)[1:20,]\nggplot(data=ep4.bigrams, aes(x=reorder(word, -freq), y=freq)) +  \n  geom_bar(stat=\"identity\", fill=\"chocolate2\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Bigram\", y=\"Frequency\")\n```\n\n# **Episode V: The Empire Strikes Back**\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# How many dialogues?\nlength(ep5$dialogue)\n\n# How many characters?\nlength(levels(ep5$character))\n\n# Top 20 characters with more dialogues \ntop.ep5.chars <- as.data.frame(sort(table(ep5$character), decreasing=TRUE))[1:20,]\n\n# Visualization \nggplot(data=top.ep5.chars, aes(x=Var1, y=Freq)) +\n  geom_bar(stat=\"identity\", fill=\"#56B4E9\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Character\", y=\"Number of dialogues\")\n```\n\n```{r eval=FALSE, message=FALSE, warning=FALSE}\n# Wordcloud for Episode V\nwordcloud2(frequentTerms(ep5$dialogue), size=0.5,\n           figPath=\"../input/wordcloud_masks/yoda.png\")\n```\n\n<img src=\"https://i.imgur.com/KMKIPMk.png\">\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Most frequent bigrams\nep5.bigrams <- frequentBigrams(ep5$dialogue)[1:20,]\nggplot(data=ep5.bigrams, aes(x=reorder(word, -freq), y=freq)) +  \n  geom_bar(stat=\"identity\", fill=\"chocolate2\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Bigram\", y=\"Frequency\")\n```\n\n# **Episode VI: Return of the Jedi**\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# How many dialogues?\nlength(ep6$dialogue)\n\n# How many characters?\nlength(levels(ep6$character))\n\n# Top 20 characters with more dialogues\ntop.ep6.chars <- as.data.frame(sort(table(ep6$character), decreasing=TRUE))[1:20,]\n\n# Visualization \nggplot(data=top.ep6.chars, aes(x=Var1, y=Freq)) +\n  geom_bar(stat=\"identity\", fill=\"#56B4E9\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Character\", y=\"Number of dialogues\")\n```\n\n```{r eval=FALSE, message=FALSE, warning=FALSE}\n# Wordcloud for Episode VI\nwordcloud2(frequentTerms(ep6$dialogue), size=0.5,\n           figPath=\"../input/wordcloud_masks/r2d2.png\")\n```\n\n<img src=\"https://i.imgur.com/0ou5EPG.png\">\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Most frequent bigrams\nep6.bigrams <- frequentBigrams(ep6$dialogue)[1:20,]\nggplot(data=ep6.bigrams, aes(x=reorder(word, -freq), y=freq)) +  \n  geom_bar(stat=\"identity\", fill=\"chocolate2\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Bigram\", y=\"Frequency\")\n```\n\n# **The Original Trilogy**\n\nIn this section we are going to compute the previous statistics, but now considering the three movies of The Original Trilogy (Episodes IV, V and VI).\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# The Original Trilogy dialogues \ntrilogy <- rbind(ep4, ep5, ep6)\n\n# How many dialogues?\nlength(trilogy$dialogue)\n\n# How many characters?\nlength(levels(trilogy$character))\n\n# Top 20 characters with more dialogues \ntop.trilogy.chars <- as.data.frame(sort(table(trilogy$character), decreasing=TRUE))[1:20,]\n\n# Visualization \nggplot(data=top.trilogy.chars, aes(x=Var1, y=Freq)) +\n  geom_bar(stat=\"identity\", fill=\"#56B4E9\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Character\", y=\"Number of dialogues\")\n```\n\nC-3PO with more dialogues than Leia and Darth Vader? Ugh... \n\n```{r eval=FALSE, message=FALSE, warning=FALSE}\n# Wordcloud for The Original Trilogy\nwordcloud2(frequentTerms(trilogy$dialogue), size=0.4)\n           figPath=\"../input/wordcloud_masks/rebel alliance.png\")\n```\n\n<img src=\"https://i.imgur.com/e7NLonz.png\">\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Most frequent bigrams\ntrilogy.bigrams <- frequentBigrams(trilogy$dialogue)[1:20,]\nggplot(data=trilogy.bigrams, aes(x=reorder(word, -freq), y=freq)) +  \n  geom_bar(stat=\"identity\", fill=\"chocolate2\", colour=\"black\") +\n  theme_bw() +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(x=\"Bigram\", y=\"Frequency\")\n```\n\n# **Sentiment analysis**\n\nLet’s address the topic of opinion mining or sentiment analysis. We can use the tools of text mining to approach the emotional content of text programmatically.\n\n```{r message=FALSE, warning=FALSE}\n# Transform the text to a tidy data structure with one token per row\ntokens <- trilogy %>%  \n  mutate(dialogue=as.character(trilogy$dialogue)) %>%\n  unnest_tokens(word, dialogue)\n```\n\nFirst we are going to use the general-purpose lexicon `bing`, from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Positive and negative words\ntokens %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort=TRUE) %>%\n  acast(word ~ sentiment, value.var=\"n\", fill=0) %>%\n  comparison.cloud(colors=c(\"#F8766D\", \"#00BFC4\"), max.words=100)\n```\n\nThe `nrc` lexicon (from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)) categorizes words in a binary fashion into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Sentiments and frequency associated with each word  \nsentiments <- tokens %>% \n  inner_join(nrc, \"word\") %>%\n  count(word, sentiment, sort=TRUE) \n\n# Frequency of each sentiment\nggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) + \n  geom_bar(stat=\"identity\", aes(fill=sentiment), show.legend=FALSE) +\n  labs(x=\"Sentiment\", y=\"Frequency\") +\n  theme_bw() \n```\n\nWe can use this lexicon to compute the most frequent words for each sentiment. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Top 10 terms for each sentiment\nsentiments %>%\n  group_by(sentiment) %>%\n  arrange(desc(n)) %>%\n  slice(1:10) %>%\n  ggplot(aes(x=reorder(word, n), y=n)) +\n  geom_col(aes(fill=sentiment), show.legend=FALSE) +\n  facet_wrap(~sentiment, scales=\"free_y\") +\n  labs(y=\"Frequency\", x=\"Terms\") +\n  coord_flip() +\n  theme_bw() \n```\n\n## Analysis by character\n\nIn the following visualizations we only consider the Top 10 characters with more dialogues. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Sentiment analysis for the Top 10 characters with more dialogues\ntokens %>%\n  filter(character %in% c(\"LUKE\",\"HAN\",\"THREEPIO\",\"LEIA\",\"VADER\",\n                          \"BEN\",\"LANDO\",\"YODA\",\"EMPEROR\",\"RED LEADER\")) %>%\n  inner_join(nrc, \"word\") %>%\n  count(character, sentiment, sort=TRUE) %>%\n  ggplot(aes(x=sentiment, y=n)) +\n  geom_col(aes(fill=sentiment), show.legend=FALSE) +\n  facet_wrap(~character, scales=\"free_x\") +\n  labs(x=\"Sentiment\", y=\"Frequency\") +\n  coord_flip() +\n  theme_bw() \n```\n\nTo calculate the most frequent words for each character, we are going to use a different approach than the term-document matrix: the tidy way. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Stopwords\nmystopwords <- data_frame(word=c(stopwords(\"english\"), \n                                 c(\"thats\",\"weve\",\"hes\",\"theres\",\"ive\",\"im\",\n                                   \"will\",\"can\",\"cant\",\"dont\",\"youve\",\"us\",\n                                   \"youre\",\"youll\",\"theyre\",\"whats\",\"didnt\")))\n\n# Tokens without stopwords\ntop.chars.tokens <- trilogy %>%\n  mutate(dialogue=as.character(trilogy$dialogue)) %>%\n  filter(character %in% c(\"LUKE\",\"HAN\",\"THREEPIO\",\"LEIA\",\"VADER\",\n                          \"BEN\",\"LANDO\",\"YODA\",\"EMPEROR\",\"RED LEADER\")) %>%\n  unnest_tokens(word, dialogue) %>%\n  anti_join(mystopwords, by=\"word\")\n\n# Most frequent words for each character\ntop.chars.tokens %>%\n  count(character, word) %>%\n  group_by(character) %>% \n  arrange(desc(n)) %>%\n  slice(1:10) %>%\n  ungroup() %>%\n  mutate(word2=factor(paste(word, character, sep=\"__\"), \n                       levels=rev(paste(word, character, sep=\"__\"))))%>%\n  ggplot(aes(x=word2, y=n)) +\n  geom_col(aes(fill=character), show.legend=FALSE) +\n  facet_wrap(~character, scales=\"free_y\") +\n  labs(x=\"Sentiment\", y=\"Frequency\") +\n  scale_x_discrete(labels=function(x) gsub(\"__.+$\", \"\", x)) +\n  coord_flip() +\n  theme_bw()\n```\n\nWhat's the problem with this visualization? Some words are generic and meaningless. We can use the `bind_tf_idf()` function\nto obtain more relevant and characteristic terms associated with each character. The idea of [`tf–idf`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (_term frequency_ - _inverse document frequency_) is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.\nIf the term appears in all documents, it is not likely to be insightful.\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Most relevant words for each character\ntop.chars.tokens %>%\n  count(character, word) %>%\n  bind_tf_idf(word, character, n) %>%\n  group_by(character) %>% \n  arrange(desc(tf_idf)) %>%\n  slice(1:10) %>%\n  ungroup() %>%\n  mutate(word2=factor(paste(word, character, sep=\"__\"), \n                       levels=rev(paste(word, character, sep=\"__\"))))%>%\n  ggplot(aes(x=word2, y=tf_idf)) +\n  geom_col(aes(fill=character), show.legend=FALSE) +\n  facet_wrap(~character, scales=\"free_y\") +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  labs(y=\"tf–idf\", x=\"Sentiment\") +\n  scale_x_discrete(labels=function(x) gsub(\"__.+$\", \"\", x)) +\n  coord_flip() +\n  theme_bw()\n```\n\nThese words are, as measured by `tf–idf`, the most important to each character.  We can identify most of them by only seeing the words. \n\n# **Summary**\n\nIn this entry we have analyzed the Star Wars scripts from The Original Trilogy Episodes by performing a statistical text analysis, including: \n\n* Most frequent words and bigrams for each Episode (term-document matrix approach).\n* Sentiment analysis using the lexicons `bing` and `nrc`, considering the three Episodes.\n* Sentiment analysis and most frequent words by character (the tidy way).\n* Most relevant words by character using the statistic `tf–idf`.\n\nIt has been a pleasure to make this post, I have learned a lot! Thank you for reading and if you like it, please upvote it. \n    \nIf you want to view another text mining notebook, you can check out the following link:\n                   \n* [Analyzing The Lord of the Rings data](https://www.kaggle.com/xvivancos/analyzing-the-lord-of-the-rings-data)\n                    \n# **References**\n\nHadley Wickham (2017). tidyverse: Easily Install and Load the 'Tidyverse'. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\n\nIngo Feinerer and Kurt Hornik (2017). tm: Text Mining Package. R package version 0.7-3. https://CRAN.R-project.org/package=tm\n\nIan Fellows (2014). wordcloud: Word Clouds. R package version 2.5. https://CRAN.R-project.org/package=wordcloud\n\nDawei Lang and Guan-tin Chien (2018). wordcloud2: Create Word Cloud by 'htmlwidget'. R package version 0.2.1. https://CRAN.R-project.org/package=wordcloud2\n\nSilge J, Robinson D (2016). “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” _JOSS_, *1*(3). doi: 10.21105/joss.00037 (URL: http://doi.org/10.21105/joss.00037), <URL: http://dx.doi.org/10.21105/joss.00037>.\n\nHadley Wickham (2007). Reshaping Data with the reshape Package. Journal of Statistical Software, 21(12), 1-20. URL http://www.jstatsoft.org/v21/i12/.\n\nDoug Ashton and Shane Porter (2016). radarchart: Radar Chart from 'Chart.js'. R package version 0.3.1. https://CRAN.R-project.org/package=radarchart\n\nHornik K, Buchta C, Zeileis A (2009). “Open-Source Machine Learning: R Meets Weka.” _Computational Statistics_, *24*(2), 225-232. doi: 10.1007/s00180-008-0119-7 (URL: http://doi.org/10.1007/s00180-008-0119-7)\n                   \nYihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.20.\n\nYihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595\n","metadata":{"_uuid":"a2b8ed48-f4ba-4bd1-bfc8-35fa53ebe666","_cell_guid":"8c6deedf-7451-432e-a54b-dabebdd61144","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}